#import <Foundation/Foundation.h>
#import "CMNsfwAdvancedResult.h"
#import "CMNsfwResult.h"
#import "CMApi.h"

/**
* imageapi
* Image Recognition and Processing APIs let you use Artificial Intelligence and Machine Learning to recognize and process images, and also perform useful image modification operations.
*
* OpenAPI spec version: v1
* 
*
* NOTE: This class is auto generated by the swagger code generator program.
* https://github.com/swagger-api/swagger-codegen.git
* Do not edit the class manually.
*/



@interface CMNsfwApi: NSObject <CMApi>

extern NSString* kCMNsfwApiErrorDomain;
extern NSInteger kCMNsfwApiMissingParamErrorCode;

-(instancetype) initWithApiClient:(CMApiClient *)apiClient NS_DESIGNATED_INITIALIZER;

/// Not safe for work (NSFW) content classification for Images
/// Classify an image into Not Safe For Work (NSFW)/Pornographic/Nudity/Racy content and Safe Content.  Helpful for filtering out unsafe content when processing user images.  Input image should be JPG, PNG or GIF.  Consumes 20 API calls.
///
/// @param imageFile Image file to perform the operation on.  Common file formats such as PNG, JPEG are supported.
/// 
///  code:200 message:"OK"
///
/// @return CMNsfwResult*
-(NSURLSessionTask*) nsfwClassifyWithImageFile: (NSURL*) imageFile
    completionHandler: (void (^)(CMNsfwResult* output, NSError* error)) handler;


/// Advanced content moderation and not safe for work (NSFW) content classification for Images
/// Uses advanced AI to classify an image into Not Safe For Work (NSFW) or not and determine if it contains nudity, graphic violence, non-graphic violence, self-harm, hate, potential illegal activity, medical imagery, or profanity.  Helpful for filtering out unsafe content when processing user images.  Input image should be JPG, PNG.  Consumes 100 API calls.  Requires Managed Instance or Private Cloud deployment, and a GPU.
///
/// @param imageFile Image file to perform the operation on.  Common file formats such as PNG, JPEG are supported.
/// 
///  code:200 message:"OK"
///
/// @return CMNsfwAdvancedResult*
-(NSURLSessionTask*) nsfwClassifyAdvancedWithImageFile: (NSURL*) imageFile
    completionHandler: (void (^)(CMNsfwAdvancedResult* output, NSError* error)) handler;


/// Not safe for work (NSFW) content classification for Documents
/// Classify a document (PDF, DOCX, DOC, XLSX, XLS, PPTX, PPT) into Not Safe For Work (NSFW)/Pornographic/Nudity/Racy content and Safe Content.  Helpful for filtering out unsafe content when processing user images.  Consumes 20 API calls per image.
///
/// @param imageFile Image file to perform the operation on.  Common file formats such as PNG, JPEG are supported.
/// 
///  code:200 message:"OK"
///
/// @return CMNsfwResult*
-(NSURLSessionTask*) nsfwClassifyDocumentWithImageFile: (NSURL*) imageFile
    completionHandler: (void (^)(CMNsfwResult* output, NSError* error)) handler;


/// Not safe for work (NSFW) content classification for Video
/// Classify a video into Not Safe For Work (NSFW)/Pornographic/Nudity/Racy content and Safe Content.  Helpful for filtering out unsafe content when processing user images.  Input image should be MP4, MOV, WEBM, MKV, AVI, FLV, MPG, GIF.  Consumes 20 API calls per frame analyzed.  Requires Cloudmersive Managed Instance or Private Cloud deployment.
///
/// @param videoFile Video file to perform the operation on.  Common file formats such as MP4, MPG are supported.
/// 
///  code:200 message:"OK"
///
/// @return CMNsfwResult*
-(NSURLSessionTask*) nsfwClassifyVideoWithVideoFile: (NSURL*) videoFile
    completionHandler: (void (^)(CMNsfwResult* output, NSError* error)) handler;



@end
