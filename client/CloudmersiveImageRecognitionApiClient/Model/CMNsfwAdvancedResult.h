#import <Foundation/Foundation.h>
#import "CMObject.h"

/**
* imageapi
* Image Recognition and Processing APIs let you use Artificial Intelligence and Machine Learning to recognize and process images, and also perform useful image modification operations.
*
* OpenAPI spec version: v1
* 
*
* NOTE: This class is auto generated by the swagger code generator program.
* https://github.com/swagger-api/swagger-codegen.git
* Do not edit the class manually.
*/





@protocol CMNsfwAdvancedResult
@end

@interface CMNsfwAdvancedResult : CMObject

/* True if the classification was successfully run, false otherwise [optional]
 */
@property(nonatomic) NSNumber* successful;
/* True if the result was clean, false otherwise [optional]
 */
@property(nonatomic) NSNumber* cleanResult;
/* True if the image contains nudity or sex, false otherwise [optional]
 */
@property(nonatomic) NSNumber* containsNudity;
/* True if the image contains graphic violence and/or gore, false otherwise [optional]
 */
@property(nonatomic) NSNumber* containsGraphicViolence;
/* True if the image contains non-graphic violence, e.g. weapons, false otherwise [optional]
 */
@property(nonatomic) NSNumber* containsNonGraphicViolence;
/* True if the image contains self-harm or suicide imagery, false otherwise [optional]
 */
@property(nonatomic) NSNumber* containsSelfHarm;
/* True if the image contains hate, false otherwise [optional]
 */
@property(nonatomic) NSNumber* containsHate;
/* True if the image contains potentially illegal activity such as drugs, false otherwise [optional]
 */
@property(nonatomic) NSNumber* containsPotentialIllegalActivity;
/* True if the image contains medical imagery, false otherwise [optional]
 */
@property(nonatomic) NSNumber* containsMedicalImagery;
/* True if the image contains profanity or obscenities, false otherwise [optional]
 */
@property(nonatomic) NSNumber* containsProfanity;
/* Score between 0.0 and 1.0.  Scores of 0.0-0.2 represent high probability safe content, while scores 0.8-1.0 represent high probability unsafe content.  Content between 0.2 and 0.8 is of increasing raciness. [optional]
 */
@property(nonatomic) NSNumber* score;
/* Classification result into four categories: SafeContent_HighProbability, UnsafeContent_HighProbability, RacyContent, SafeContent_ModerateProbability [optional]
 */
@property(nonatomic) NSString* classificationOutcome;

@end
